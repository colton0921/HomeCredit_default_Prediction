{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Business Question\n",
    "# 2) Data Collection\n",
    "# 3) Preprocessing\n",
    "# 4) Model(s) Creation\n",
    "# 5) Model Evaluation and Comparison\n",
    "# 6) Conclusion and Future Improvements\n",
    "#https://www.kaggle.com/c/home-credit-default-risk/data?select=installments_payments.csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import scipy\n",
    "from scipy import stats\n",
    "#Graph\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from  sklearn.impute import SimpleImputer   \n",
    "from imblearn.over_sampling import SMOTE\n",
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Correlation preset - part 1\n",
    "# df = pd.read_csv(\"./data/application_train.csv\" )\n",
    "# df_corr = pd.DataFrame( df.corr()['TARGET'])\n",
    "# df_corr['MissingValue'] = [df[list(df_corr.index)[x]].isna().sum() for x in range(len(pd.DataFrame(df_corr)['TARGET']))]\n",
    "# #df_corr = df_corr[df_corr['MissingValue']>30000].sort_values(by=['TARGET'])\n",
    "# df_corr['Abs_correlation'] = abs(df_corr['TARGET'])\n",
    "# df_corr.sort_values(by=['Abs_correlation'],ascending=False,inplace=True)\n",
    "# print(df_corr)\n",
    "# print(len(list(df_corr.index)))\n",
    "# \"','\".join(list(df_corr.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Correlation preset - part 2\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "# #Keep certain percent\n",
    "# df_corr_Work = df_corr[(df_corr['TARGET']>=-0.04)&(df_corr['TARGET']<=0.04)]\n",
    "# print('shape: ',df_corr_Work.shape)\n",
    "# print(df_corr_Work)\n",
    "# print(list(df_corr_Work.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "original shape:  (307511, 122)\n",
      "Columns that is dropped due to missing values: \n",
      " [] []\n",
      "df shape before outliners: (307511, 121)\n",
      "df shape after outliners: (151404, 121)\n",
      "0.0    37443\n",
      "1.0    12481\n",
      "Name: TARGET, dtype: int64\n",
      "X.shape =  (49924, 225)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  \\\n",
       "8            0.0          0.109707    0.554188     0.466236         0.543662   \n",
       "9            0.0          0.138426    0.204705     0.267475         0.225352   \n",
       "13           0.5          0.253303    0.496676     0.395085         0.408451   \n",
       "21           0.5          0.224584    0.618422     0.520917         0.552113   \n",
       "22           0.5          0.540494    0.257314     0.447131         0.253521   \n",
       "\n",
       "    REGION_POPULATION_RELATIVE  DAYS_BIRTH  DAYS_EMPLOYED  DAYS_REGISTRATION  \\\n",
       "8                     0.396196    0.291120       1.000000           0.522134   \n",
       "9                     0.419288    0.612430       0.040503           0.071098   \n",
       "13                    0.351938    0.634288       0.037867           0.958628   \n",
       "21                    0.539125    0.592683       0.041461           0.852078   \n",
       "22                    0.441701    0.802077       0.034528           0.992665   \n",
       "\n",
       "    DAYS_ID_PUBLISH  ...  FONDKAPREMONT_MODE_reg oper spec account  \\\n",
       "8          0.511741  ...                                         0   \n",
       "9          0.445324  ...                                         0   \n",
       "13         0.317632  ...                                         0   \n",
       "21         0.680561  ...                                         0   \n",
       "22         0.650132  ...                                         0   \n",
       "\n",
       "    HOUSETYPE_MODE_specific housing  HOUSETYPE_MODE_terraced house  \\\n",
       "8                                 0                              0   \n",
       "9                                 0                              0   \n",
       "13                                0                              0   \n",
       "21                                0                              0   \n",
       "22                                0                              0   \n",
       "\n",
       "    WALLSMATERIAL_MODE_Mixed  WALLSMATERIAL_MODE_Monolithic  \\\n",
       "8                          0                              0   \n",
       "9                          0                              0   \n",
       "13                         0                              0   \n",
       "21                         0                              0   \n",
       "22                         0                              0   \n",
       "\n",
       "    WALLSMATERIAL_MODE_Others  WALLSMATERIAL_MODE_Panel  \\\n",
       "8                           0                         1   \n",
       "9                           0                         1   \n",
       "13                          0                         1   \n",
       "21                          0                         1   \n",
       "22                          0                         1   \n",
       "\n",
       "    WALLSMATERIAL_MODE_Stone, brick  WALLSMATERIAL_MODE_Wooden  \\\n",
       "8                                 0                          0   \n",
       "9                                 0                          0   \n",
       "13                                0                          0   \n",
       "21                                0                          0   \n",
       "22                                0                          0   \n",
       "\n",
       "    EMERGENCYSTATE_MODE_Yes  \n",
       "8                         0  \n",
       "9                         0  \n",
       "13                        0  \n",
       "21                        0  \n",
       "22                        0  \n",
       "\n",
       "[5 rows x 225 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CNT_CHILDREN</th>\n      <th>AMT_INCOME_TOTAL</th>\n      <th>AMT_CREDIT</th>\n      <th>AMT_ANNUITY</th>\n      <th>AMT_GOODS_PRICE</th>\n      <th>REGION_POPULATION_RELATIVE</th>\n      <th>DAYS_BIRTH</th>\n      <th>DAYS_EMPLOYED</th>\n      <th>DAYS_REGISTRATION</th>\n      <th>DAYS_ID_PUBLISH</th>\n      <th>...</th>\n      <th>FONDKAPREMONT_MODE_reg oper spec account</th>\n      <th>HOUSETYPE_MODE_specific housing</th>\n      <th>HOUSETYPE_MODE_terraced house</th>\n      <th>WALLSMATERIAL_MODE_Mixed</th>\n      <th>WALLSMATERIAL_MODE_Monolithic</th>\n      <th>WALLSMATERIAL_MODE_Others</th>\n      <th>WALLSMATERIAL_MODE_Panel</th>\n      <th>WALLSMATERIAL_MODE_Stone, brick</th>\n      <th>WALLSMATERIAL_MODE_Wooden</th>\n      <th>EMERGENCYSTATE_MODE_Yes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>0.0</td>\n      <td>0.109707</td>\n      <td>0.554188</td>\n      <td>0.466236</td>\n      <td>0.543662</td>\n      <td>0.396196</td>\n      <td>0.291120</td>\n      <td>1.000000</td>\n      <td>0.522134</td>\n      <td>0.511741</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.0</td>\n      <td>0.138426</td>\n      <td>0.204705</td>\n      <td>0.267475</td>\n      <td>0.225352</td>\n      <td>0.419288</td>\n      <td>0.612430</td>\n      <td>0.040503</td>\n      <td>0.071098</td>\n      <td>0.445324</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.5</td>\n      <td>0.253303</td>\n      <td>0.496676</td>\n      <td>0.395085</td>\n      <td>0.408451</td>\n      <td>0.351938</td>\n      <td>0.634288</td>\n      <td>0.037867</td>\n      <td>0.958628</td>\n      <td>0.317632</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.5</td>\n      <td>0.224584</td>\n      <td>0.618422</td>\n      <td>0.520917</td>\n      <td>0.552113</td>\n      <td>0.539125</td>\n      <td>0.592683</td>\n      <td>0.041461</td>\n      <td>0.852078</td>\n      <td>0.680561</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.5</td>\n      <td>0.540494</td>\n      <td>0.257314</td>\n      <td>0.447131</td>\n      <td>0.253521</td>\n      <td>0.441701</td>\n      <td>0.802077</td>\n      <td>0.034528</td>\n      <td>0.992665</td>\n      <td>0.650132</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 225 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "##                                      (Drop columns with missing data & 4% correlation)\n",
    "# SMOTE                                 (Imbalanced data: score 15 points more)\n",
    "# Outliner   >>> (remove 3% standard deviation)\n",
    "# Mean on numbers; Median on labels     (Median performs little worse)  diff show outliners\n",
    "# drop_rate = 1                         (Drop nothing to performs little better))\n",
    "# Standardization                       (MinMaxScaler performs little worse))\n",
    "# Regression                            (Score 68)\n",
    "# Other Models: XGboost\n",
    "\n",
    "#Load DataFrame\n",
    "df = pd.read_csv(\"./data/application_train.csv\" ) #  index_col=0  ID which is used for mapping other excel files goes to index column\n",
    "\n",
    "print('original shape: ' , df.shape)\n",
    "\n",
    "\n",
    "#Step 1:    Drop columns with missing values  #---------------------Data Preprocessing---------------------#\n",
    "\n",
    "#drop ID\n",
    "df.drop(columns=['SK_ID_CURR'], inplace=True)\n",
    "\n",
    "#Step 2:    Fill in missing values            #---------------------Data Preprocessing---------------------#\n",
    "\n",
    "#   >>>categorical columns<<<   \n",
    "drop_rate = 1 #0.3 mean the data with 70% more  \n",
    "drop_col_lst_1 = []\n",
    "for col in df.columns:\n",
    "    if (df[col].isna().sum() / df.shape[0] <= drop_rate )&(type(df[col][0])==str):\n",
    "        df[col].fillna(  df[col].mode()[0]  ,inplace=True)  #   >>>categorical columns<<<   Minor missing values\n",
    "    elif (df[col].isna().sum() / df.shape[0] > drop_rate )&(type(df[col][0])==str):\n",
    "        drop_col_lst_1.append(col)                          #   >>>categorical columns<<<   Huge missing values\n",
    "df.drop(columns=drop_col_lst_1 , inplace=True)\n",
    "\n",
    "#   >>>Numerical columns<<<   \n",
    "drop_col_lst_2 = []\n",
    "from sklearn.impute import SimpleImputer\n",
    "for col in df.columns:\n",
    "    if (df[col].isna().sum() / df.shape[0] > drop_rate )&((type(df[col][0])==np.int64)|(type(df[col][0])==np.float64)):\n",
    "        drop_col_lst_2.append(col) \n",
    "print('Columns that is dropped due to missing values: \\n',drop_col_lst_1,drop_col_lst_2)\n",
    "df.drop(columns=drop_col_lst_2 , inplace=True)\n",
    "#   Replace missing values with mean\n",
    "df[list(df.select_dtypes(include=[\"int64\",\"float64\"]).columns)]     =       pd.DataFrame(   SimpleImputer(missing_values=np.NaN, strategy='mean').fit_transform(  df[list(df.select_dtypes(include=[\"int64\",\"float64\"]).columns)].values),index = df.index,columns=  list(df.select_dtypes(include=[\"int64\",\"float64\"]).columns) )   \n",
    "\n",
    "\n",
    "#Step 0:    fix unbalanced dataset (SMOTE)    #---------------------Data Preprocessing---------------------#\n",
    "print('df shape before outliners:' , df.shape)\n",
    "df = df[(np.abs( df[df.drop(columns='TARGET').select_dtypes(include=[np.number]).columns].apply(stats.zscore))       < float(3)).all(axis=1)]\n",
    "print('df shape after outliners:' , df.shape)\n",
    "\n",
    "df.drop(df.query('TARGET == 0').sample(((df.query('TARGET == 0').shape[0])-3*(df.query('TARGET == 1').shape[0]))).index,inplace=True)  #Downscale the data with '0'\n",
    "print(df.TARGET.value_counts())\n",
    "#Step 3:    Standardization & Labelling       #---------------------Data Preprocessing---------------------#\n",
    "\n",
    "X = df.drop(columns=['TARGET'])\n",
    "y = df[['TARGET']]\n",
    "\n",
    "# Standardization (0 mean, 1 stdev)\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# X[list(X.select_dtypes(include=[\"int64\",\"float64\"]).columns)] = pd.DataFrame(   StandardScaler().fit_transform(X[list(X.select_dtypes(include=[\"int64\",\"float64\"]).columns)].values), index = X.index, columns = list(X[list(X.select_dtypes(include=[\"int64\",\"float64\"]).columns)].columns))   \n",
    "#https://stackoverflow.com/questions/35723472/how-to-use-sklearn-fit-transform-with-pandas-and-return-dataframe-instead-of-num\n",
    "\n",
    "#MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X[list(X.select_dtypes(include=[\"int64\",\"float64\"]).columns)] = pd.DataFrame(   MinMaxScaler(feature_range=(0, 1)).fit_transform(X[list(X.select_dtypes(include=[\"int64\",\"float64\"]).columns)].values), index = X.index, columns = list(X[list(X.select_dtypes(include=[\"int64\",\"float64\"]).columns)].columns)) \n",
    "\n",
    "X= pd.get_dummies(X, drop_first=True)       #One Hot Encoder  #drop_first=True -> reduce one column\n",
    "\n",
    "\n",
    "#Step 4:    fix unbalanced dataset (SMOTE)    #---------------------Data Preprocessing---------------------#\n",
    "#df.drop(df.query('TARGET == 0').sample((282686-24825)).index,inplace=True)  #Sample size of '0' is 282686; Sample size of '1' is 24825; \n",
    "\n",
    "#Oversampling the data     ## https://towardsdatascience.com/5-smote-techniques-for-oversampling-your-imbalance-data-b8155bdbe2b5\n",
    "\n",
    "print('X.shape = ' , X.shape)\n",
    "y.head()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "confusion_matrix:    \n [[7114  375]\n [1886  610]]\n\nTrue positive      False positive\nFalse Negative      True negative\n\nclassification_report:   \n               precision    recall  f1-score   support\n\n         0.0       0.79      0.95      0.86      7489\n         1.0       0.62      0.24      0.35      2496\n\n    accuracy                           0.77      9985\n   macro avg       0.70      0.60      0.61      9985\nweighted avg       0.75      0.77      0.73      9985\n\naccuracy_score:  \n 0.7735603405107662  ; with data size of  (39939, 225)\n"
     ]
    }
   ],
   "source": [
    "#Step 4:    Build regression model       #---------------------Modelling---------------------#\n",
    "\n",
    "# Import module to split dataset------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data set into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)       #\n",
    "\n",
    "\n",
    "# print('Sample size before SMOTE method: ' , X_train.shape)\n",
    "# X_train,y_train = SMOTE(random_state = 42).fit_resample(X_train,y_train)\n",
    "# print('Sample size after SMOTE method: ' , X_train.shape)\n",
    "\n",
    "# Import module for fitting------------------------------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create instance (i.e. object) of LogisticRegression\n",
    "logmodel = LogisticRegression()\n",
    "\n",
    "# Fit the model using the training data\n",
    "logmodel.fit(X_train, y_train)\n",
    "#print('coef :  ' , logmodel.coef_)\n",
    "#print('intercept :  ', logmodel.intercept_)\n",
    "\n",
    "#Predicting the target for test data\n",
    "y_pred = logmodel.predict(X_test)\n",
    "\n",
    "#Classification Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('confusion_matrix:    \\n',    confusion_matrix(y_test, y_pred))\n",
    "print('''\n",
    "True positive      False positive\n",
    "False Negative      True negative\n",
    "''')\n",
    "from sklearn.metrics import classification_report\n",
    "print('classification_report:   \\n',    classification_report(y_test, y_pred))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy_score:  \\n',    accuracy_score(y_test, y_pred) , ' ; with data size of ' , X_train.shape)     #Same as logmodel.score(X_test,y_test)\n",
    "#--Other models--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.684692849949647\n",
    "#Step 5:    Build Forest model with PCA  #---------------------Modelling---------------------# \n",
    "# XGboost (no need scaling, no need worry missing data)\n",
    "# parameter >  give class\n",
    "#Outliner\n",
    "#SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "            0         1         2         3         4         5         6  \\\n0   -3.028981 -0.413400 -1.540603 -1.940079 -0.240909  0.408641 -0.198666   \n1   -0.357759 -0.446519  0.265239  0.260461  0.615010 -0.399651  0.147452   \n2    0.911451  0.438051  1.633826  1.451226  0.240125 -0.245653 -2.585751   \n3    2.389625  0.333127  0.616318  2.045917 -0.424432 -1.531849 -2.172485   \n4    1.472375  1.214850 -0.112170  1.236217  1.885997 -1.226767 -2.179582   \n..        ...       ...       ...       ...       ...       ...       ...   \n995 -1.773791  1.548493 -0.071203 -0.135549 -0.945316  0.613645  0.820349   \n996 -0.510020  0.171239  0.121757  0.626236 -0.170996  0.006801 -0.644402   \n997 -2.816729 -0.763501 -0.212769 -1.413166  2.395144 -1.306546  1.027126   \n998  0.182859 -0.339780  0.488050  0.252066  1.106527 -1.226481 -2.388367   \n999 -0.763217  2.125145 -1.275344  0.010466  0.756689  1.608059 -1.204031   \n\n            7         8         9        10        11        12        13  \\\n0   -1.115794 -2.028031 -1.818871 -0.538178  1.088236 -0.082524 -0.009153   \n1    0.051481  0.319822 -0.754620 -0.715564 -0.368849 -1.946864  0.768768   \n2   -0.827761  0.523011 -0.739338 -0.764926  0.090622  0.603169 -2.232015   \n3   -0.362803  0.656756  1.780361 -0.779424 -1.123244  1.073439  1.311648   \n4    0.608373  0.651458  0.381252  0.238339 -0.937426  0.804628  0.133330   \n..        ...       ...       ...       ...       ...       ...       ...   \n995  0.081977  0.757424  0.004659 -0.273800 -0.336797 -0.468534  0.967175   \n996 -1.492353 -1.610849  0.599232 -0.088139 -0.305770  0.476205  0.001618   \n997 -0.942757  1.011138 -0.930459 -0.675991 -0.787553  0.256596  1.354824   \n998 -0.462002 -0.938749 -0.394483  0.293219  0.388129 -0.384147 -0.889126   \n999 -0.626053 -0.309124 -0.044075 -0.732265 -0.240970 -0.957953 -1.126714   \n\n           14        15        16        17        18        19  \n0   -1.605804 -0.855536  0.034564 -1.290017 -0.600121 -1.297529  \n1   -0.917956 -0.964851  1.294147  1.000935  0.173198 -1.654220  \n2   -1.077177 -0.755529 -0.401290  0.415035  3.183995 -1.403174  \n3   -0.051616 -1.734877  1.290252 -0.425479  2.543703  0.828011  \n4   -0.131533 -1.239884  0.190286  1.742187  2.211598  0.224298  \n..        ...       ...       ...       ...       ...       ...  \n995 -2.122231 -0.846478  0.494028  0.723937 -0.070907  1.612784  \n996 -1.687819 -1.138632 -0.377692 -0.899806  1.297382  0.691098  \n997 -1.893415 -1.174344  0.175296  0.168663 -1.204457  0.182146  \n998 -0.637572 -1.622800 -2.585421  0.843056  1.877571 -0.058161  \n999 -1.050245  0.455079 -0.745386  0.978752  1.510448 -1.396823  \n\n[1000 rows x 20 columns]\nfeature_importances_ [0.21521153 0.01054557 0.00689419 0.17404434 0.00414734 0.00704686\n 0.19761999 0.01865445 0.00608294 0.00490484 0.00866699 0.0046718\n 0.00359038 0.01168016 0.09392572 0.04978297 0.0033278  0.01008566\n 0.15534173 0.01377474]\n[1]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_classes=3,\n",
    "                           n_informative=4, weights=[0.2, 0.3, 0.5],\n",
    "                           random_state=0)\n",
    "                        \n",
    "clf = BalancedRandomForestClassifier(max_depth=2, random_state=0)\n",
    "clf.fit(X, y)  \n",
    "print(pd.DataFrame(X))\n",
    "print('feature_importances_' , clf.feature_importances_)  \n",
    "print(clf.predict([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0    111138\n",
      "1.0      9985\n",
      "Name: TARGET, dtype: int64\n",
      "0.0    27785\n",
      "1.0     2496\n",
      "Name: TARGET, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Training target statistics: Counter({'TARGET': 1})\n",
      "Testing target statistics: Counter({'TARGET': 1})\n",
      "confusion_matrix:    \n",
      " [[25306  2479]\n",
      " [ 2089   407]]\n",
      "\n",
      "True positive      False positive\n",
      "False Negative      True negative\n",
      "\n",
      "classification_report:   \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.91      0.92     27785\n",
      "         1.0       0.14      0.16      0.15      2496\n",
      "\n",
      "    accuracy                           0.85     30281\n",
      "   macro avg       0.53      0.54      0.53     30281\n",
      "weighted avg       0.86      0.85      0.85     30281\n",
      "\n",
      "accuracy_score:  \n",
      " 0.8491463293814603  ; with data size of  (121123, 227)\n"
     ]
    }
   ],
   "source": [
    "#Step 4:    Split data                    #---------------------Modelling---------------------#\n",
    "\n",
    "# Import module to split dataset------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data set into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42 , stratify=y)       #\n",
    "\n",
    "print(y_train['TARGET'].value_counts())\n",
    "print(y_test['TARGET'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "# print('Sample size before SMOTE method: ' , X_train.shape)\n",
    "# X_train,y_train = SMOTE(random_state = 42).fit_resample(X_train,y_train)\n",
    "# print('Sample size after SMOTE method: ' , X_train.shape)\n",
    "\n",
    "#Step 5:    Build            model       #---------------------Modelling---------------------#  RamdonForest\n",
    "\n",
    "# # Import module for fitting------------------------------------\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# rnd_clf = RandomForestClassifier(n_estimators=2000, n_jobs=-1, random_state=42)  #, max_leaf_nodes=16\n",
    "# rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = rnd_clf.predict(X_test)\n",
    "#Step 5:    Build            model       #---------------------Modelling---------------------#  Multiclass classification with under-sampling\n",
    "\n",
    "# print(type(y_train))\n",
    "# print('Training target statistics: {}'.format(Counter(y_train)))\n",
    "# print('Testing target statistics: {}'.format(Counter(y_test)))\n",
    "\n",
    "# # Create a pipeline\n",
    "# pipeline = make_pipeline(NearMiss(version=2),\n",
    "#                          LinearSVC(random_state=42))\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "#Step 5:    Build regression model       #---------------------Modelling---------------------#  RamdonForest > unlimitted depth and leaf\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    #DecisionTreeClassifier(max_depth=2), n_estimators=200,learning_rate=0.05, random_state=42)\n",
    "    DecisionTreeClassifier(), n_estimators=200,learning_rate=0.05, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "\n",
    "# # #5.1 - Hyperparameter Tuning (Max Depth)#---------------------Modelling---------------------#  Find Max depth\n",
    "# # parameters = {'max_depth' : list(range(1,11))}\n",
    "# # from sklearn.model_selection import GridSearchCV\n",
    "# # search = GridSearchCV(DecisionTreeClassifier(),param_grid=parameters,cv=10)\n",
    "# # search.fit(X_train,y_train)\n",
    "# # print('search.best_score_:' , search.best_score_)\n",
    "# # print('search.best_params_:', search.best_params_)\n",
    "\n",
    "# # #5.2 - Hyperparameter Tuning (Max Depth)#---------------------Modelling---------------------#  List all score of diff depth\n",
    "# # # List of values to try for max_depth:\n",
    "# # max_depth_range = list(range(1, 11))\n",
    "# # # List to store the accuracy for each value of max_depth:\n",
    "# # accuracy = []\n",
    "# # for depth in max_depth_range:\n",
    "    \n",
    "# #     clf = DecisionTreeClassifier(max_depth = depth, random_state=0)\n",
    "# #     clf.fit(X_train, y_train)\n",
    "# #     score = clf.score(X_test, y_test)\n",
    "# #     accuracy.append(score)\n",
    "\n",
    "#Step 6:    Classification Metrics        #---------------------Score---------------------#\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('confusion_matrix:    \\n',    confusion_matrix(y_test, y_pred))\n",
    "print('''\n",
    "True positive      False positive\n",
    "False Negative      True negative\n",
    "''')\n",
    "from sklearn.metrics import classification_report\n",
    "print('classification_report:   \\n',    classification_report(y_test, y_pred))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy_score:  \\n',    accuracy_score(y_test, y_pred) , ' ; with data size of ' , X_train.shape)     #Same as logmodel.score(X_test,y_test)\n",
    "#--Other models--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Automatically created module for IPython interactive environment\nTraining target statistics: Counter({1: 38, 2: 38, 0: 17})\nTesting target statistics: Counter({1: 12, 2: 12, 0: 8})\n                   pre       rec       spe        f1       geo       iba       sup\n\n          0       1.00      1.00      1.00      1.00      1.00      1.00         8\n          1       1.00      0.83      1.00      0.91      0.91      0.82        12\n          2       0.86      1.00      0.90      0.92      0.95      0.91        12\n\navg / total       0.95      0.94      0.96      0.94      0.95      0.90        32\n\n"
     ]
    }
   ],
   "source": [
    "#https://imbalanced-learn.org/stable/auto_examples/applications/plot_multi_class_under_sampling.html\n",
    "# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n",
    "# License: MIT\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.datasets import make_imbalance\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Create a folder to fetch the dataset\n",
    "iris = load_iris()\n",
    "X, y = make_imbalance(iris.data, iris.target,\n",
    "                      sampling_strategy={0: 25, 1: 50, 2: 50},\n",
    "                      random_state=RANDOM_STATE)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=RANDOM_STATE)\n",
    "\n",
    "print('Training target statistics: {}'.format(Counter(y_train)))\n",
    "print('Testing target statistics: {}'.format(Counter(y_test)))\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = make_pipeline(NearMiss(version=2),\n",
    "                         LinearSVC(random_state=RANDOM_STATE))\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Classify and report the results\n",
    "print(classification_report_imbalanced(y_test, pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current value: num_threads=-1\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "LightGBMError",
     "evalue": "Do not support special JSON characters in feature name.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-404c5deeae33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m lgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], \n\u001b[1;32m---> 46\u001b[1;33m         eval_metric= 'auc', verbose= 200, early_stopping_rounds= 100)\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfolds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m  \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    855\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m                                         callbacks=callbacks, init_model=init_model)\n\u001b[0m\u001b[0;32m    858\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    615\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_metrics_callable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m                               callbacks=callbacks, init_model=init_model)\n\u001b[0m\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;31m# construct booster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   2051\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2052\u001b[0m             \u001b[1;31m# construct booster object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2053\u001b[1;33m             \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2054\u001b[0m             \u001b[1;31m# copy the parameters from train_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2055\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1323\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m                                 \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1325\u001b[1;33m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[0;32m   1326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m   1149\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Wrong predictor type {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;31m# set feature names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_feature_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init_from_np2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mset_feature_name\u001b[1;34m(self, feature_name)\u001b[0m\n\u001b[0;32m   1630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m                 \u001b[0mc_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_feature_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1632\u001b[1;33m                 ctypes.c_int(len(feature_name))))\n\u001b[0m\u001b[0;32m   1633\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \"\"\"\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Do not support special JSON characters in feature name."
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from lightgbm import LGBMClassifier\n",
    "# folds = StratifiedKFold(n_splits= 10, shuffle=True, random_state=2020)\n",
    "# sub_preds = np.zeros(df_test.shape[0])\n",
    "# feats = [f for f in df_train.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "\n",
    "# for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df_train[feats], df_train['TARGET'])):\n",
    "#     train_x, train_y = df_train[feats].iloc[train_idx], df_train['TARGET'].iloc[train_idx]\n",
    "#     valid_x, valid_y = df_train[feats].iloc[valid_idx], df_train['TARGET'].iloc[valid_idx]\n",
    "\n",
    "#     lgb = LGBMClassifier(nthread=4, n_estimators=12000, learning_rate=0.02, num_leaves=31,\n",
    "#         colsample_bytree=0.85,subsample=0.9, max_depth=8, reg_alpha=0.0415, reg_lambda=0.073,\n",
    "#         min_split_gain=0.022, min_child_weight=39.32, silent=-1, verbose=-1)\n",
    "\n",
    "#     lgb.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "#         eval_metric= 'auc', verbose= 200, early_stopping_rounds= 100)\n",
    "\n",
    "#     sub_preds += lgb.predict_proba(df_test[feats], num_iteration=lgb.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "#     del lgb, train_x, train_y, valid_x, valid_y\n",
    "#     gc.collect()\n",
    "\n",
    "# df_test['TARGET'] = sub_preds\n",
    "\n",
    "#Step 4:    Split data                    #---------------------Modelling---------------------#\n",
    "\n",
    "# Import module to split dataset------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data set into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42 , stratify=y)       #\n",
    "\n",
    "#Step 5:   LGBMClassifier                    #---------------------Modelling---------------------#\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "folds = StratifiedKFold(n_splits= 10, shuffle=True, random_state=2020)\n",
    "#sub_preds = np.zeros(df_test.shape[0])\n",
    "\n",
    "\n",
    "lgb = LGBMClassifier(nthread=4, n_estimators=12000, learning_rate=0.02, num_leaves=31,\n",
    "        colsample_bytree=0.85,subsample=0.9, max_depth=8, reg_alpha=0.0415, reg_lambda=0.073,\n",
    "        min_split_gain=0.022, min_child_weight=39.32, silent=-1, verbose=-1)\n",
    "\n",
    "lgb.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], \n",
    "        eval_metric= 'auc', verbose= 200, early_stopping_rounds= 100)\n",
    "\n",
    "print( lgb.predict_proba(X, num_iteration=lgb.best_iteration_)[:, 1] / folds.n_splits  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-f89d2bf393af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[^A-Za-z0-9_]+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[0;32m    868\u001b[0m         \u001b[1;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m         result = self.predict_proba(X, raw_score, start_iteration, num_iteration,\n\u001b[1;32m--> 870\u001b[1;33m                                     pred_leaf, pred_contrib, **kwargs)\n\u001b[0m\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_objective\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mraw_score\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpred_leaf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[0;32m    921\u001b[0m         \"\"\"\n\u001b[0;32m    922\u001b[0m         result = super(LGBMClassifier, self).predict(X, raw_score, start_iteration, num_iteration,\n\u001b[1;32m--> 923\u001b[1;33m                                                      pred_leaf, pred_contrib, **kwargs)\n\u001b[0m\u001b[0;32m    924\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_objective\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mraw_score\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpred_leaf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m             warnings.warn(\"Cannot compute class probabilities or labels \"\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, **kwargs)\u001b[0m\n\u001b[0;32m    686\u001b[0m                              % (self._n_features, n_features))\n\u001b[0;32m    687\u001b[0m         return self._Booster.predict(X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration,\n\u001b[1;32m--> 688\u001b[1;33m                                      pred_leaf=pred_leaf, pred_contrib=pred_contrib, **kwargs)\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, **kwargs)\u001b[0m\n\u001b[0;32m   2957\u001b[0m         return predictor.predict(data, start_iteration, num_iteration,\n\u001b[0;32m   2958\u001b[0m                                  \u001b[0mraw_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2959\u001b[1;33m                                  data_has_header, is_reshape)\n\u001b[0m\u001b[0;32m   2960\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2961\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrefit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, start_iteration, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_data_from_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpandas_categorical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[0mpredict_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC_API_PREDICT_NORMAL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mraw_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m             \u001b[0mpredict_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC_API_PREDICT_RAW_SCORE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpred_leaf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1477\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m         raise ValueError(\n\u001b[1;32m-> 1479\u001b[1;33m             \u001b[1;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1480\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1481\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "\n",
    "# Import module to split dataset------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data set into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42 , stratify=y)       #\n",
    "\n",
    "import re\n",
    "X_train = X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "opt_parameters = {'colsample_bytree': 0.9234, 'min_child_samples': 399, 'min_child_weight': 0.1, 'num_leaves': 13, 'reg_alpha': 2, 'reg_lambda': 5, 'subsample': 0.855,'imbalanced':True}\n",
    "\n",
    "clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000, colsample_bytree= 0.9234, min_child_samples= 399, min_child_weight= 0.1, num_leaves=13, reg_alpha= 2, reg_lambda= 5,subsample= 0.855,imbalanced=True)\n",
    "#clf_final = lgb.LGBMClassifier(**clf.get_params())\n",
    "#set optimal parameters\n",
    "#clf_final.set_params(**opt_parameters)\n",
    "\n",
    "#Train the final model with learning rate decay\n",
    "#clf_final.fit(X_train, y_train, **fit_params, callbacks=[lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_0995)])\n",
    "#clf_final.fit(X_train, y_train)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "X_test = X_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "y_pred = clf.predict(X_test,y_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('confusion_matrix:    \\n',    confusion_matrix(y_test, y_pred))\n",
    "print('''\n",
    "True positive      False positive\n",
    "False Negative      True negative\n",
    "''')\n",
    "from sklearn.metrics import classification_report\n",
    "print('classification_report:   \\n',    classification_report(y_test, y_pred))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy_score:  \\n',    accuracy_score(y_test, y_pred) , ' ; with data size of ' , X_train.shape)     #Same as logmodel.score(X_test,y_test)\n",
    "#--Other models--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}