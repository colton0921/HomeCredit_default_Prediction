{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle dataset based on Credit Loan applications for homes. Fictitious client data correspond to the rows and dataset indicates whether or not these clients defaulted or on their loans or not. Our aim is to analyse and create a model that successfully predicts with certain degree of accuracy future or potential loan defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = pd.read_csv('./data/application_train.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(((credit['TARGET'])==0).sum())\n",
    "print(((credit['TARGET'])==1).sum())\n",
    "imbalance = (((credit['TARGET'])==1).sum()/((credit['TARGET'])==0).sum()*100)\n",
    "imbalance\n",
    "#checking for data imbalance\n",
    "#highly imbalanced data, 9:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contained a highly imbalanced dataset. Our models were initally predicting with a misleadingly high accuracy of 98%.\n",
    "we looked to methods in solving this issue; most simply we randomly removed a chunk of the data so that sample size for TARGET==1 was the same as TARGET==0 so the trained data had an equal sample to train on; thereby creating a realistic representation on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=credit.drop(credit.query('TARGET == 0').sample(257861).index)\n",
    "df.shape\n",
    "#equalize sample size for training\n",
    "#randomly remove a sample of x amount of rows that have target ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n",
    "df.isnull().sum()\n",
    "#check for na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"Your selected dataframe has \" + str(default.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        return mis_val_table_ren_columns\n",
    "\n",
    "missing_values = missing_values_table(df)\n",
    "missing_values\n",
    "    #which collumns have missing data"
   ]
  },
  {
   "source": [
    "finding the correlation of each feature columns to the TARGET"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get corr\n",
    "df = pd.read_csv(\"application_train.csv\" )\n",
    "df_corr = pd.DataFrame( df.corr()['TARGET'])\n",
    "df_corr['MissingValue'] = [df[list(df_corr.index)[x]].isna().sum() for x in range(len(pd.DataFrame(df_corr)['TARGET']))]\n",
    "#df_corr = df_corr[df_corr['MissingValue']>30000].sort_values(by=['TARGET'])\n",
    "df_corr['Abs_correlation'] = abs(df_corr['TARGET'])\n",
    "df_corr.sort_values(by=['Abs_correlation'],ascending=False,inplace=True)\n",
    "\n",
    "#get top 20\n",
    "df_corr_top20 = df_corr[(df_corr['Abs_correlation']>=0.034199)]\n",
    "df_corr_top20.drop('TARGET',inplace=True)\n",
    "df_corr_top20.columns=['Correlation','Count_MissingValues','Abs_correlation']\n",
    "df_corr_top20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use of heatmap to look for obvious patterns to assist us in finding correlation on potential features to focus on or remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20 = list(df_corr_top20.index)\n",
    "df_top20 = df[top20]\n",
    "df_top20.insert(0, 'TARGET', df['TARGET'])\n",
    "plt.figure(figsize=(40,12))\n",
    "sns.heatmap(df_top20.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA - Plotting the various features against the trend to identify the ones that have the most correlation and gain further insight to refine the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(feature,label_rotation=True,horizontal_layout=True):\n",
    "    temp = credit[feature].value_counts()\n",
    "    df1 = pd.DataFrame({feature: temp.index,'Number of contracts': temp.values})\n",
    "\n",
    "    # Calculate the percentage of target=1 per category value\n",
    "    cat_perc = credit[[feature, 'TARGET']].groupby([feature],as_index=False).mean()\n",
    "    cat_perc.sort_values(by='TARGET', ascending=False, inplace=True)\n",
    "    \n",
    "    if(horizontal_layout):\n",
    "        fig, (ax2) = plt.subplots(ncols=1, figsize = (12,8))\n",
    "    else:\n",
    "        fig, (ax2) = plt.subplots(nrows=1, figsize =(12,8))\n",
    "        sns.set_color_codes(\"pastel\")\n",
    "        #s = sns.barplot(ax=ax1, x = feature, y=\"Number of contracts\",data=df1)\n",
    "    #if(label_rotation):\n",
    "        #s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "    \n",
    "    s = sns.barplot(ax=ax2, x = feature, y='TARGET', order=cat_perc[feature], data=cat_perc)\n",
    "    if(label_rotation):\n",
    "        s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "    plt.ylabel('Percent of target with value 1 [%]', fontsize=10)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats('NAME_CONTRACT_TYPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats('CODE_GENDER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats('FLAG_OWN_CAR')\n",
    "plot_stats('FLAG_OWN_REALTY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats('NAME_FAMILY_STATUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats('NAME_INCOME_TYPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats('OCCUPATION_TYPE',True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(\"FLAG_OWN_CAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(\"NAME_INCOME_TYPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats('NAME_EDUCATION_TYPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(\"NAME_HOUSING_TYPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Distribution of AMT_CREDIT\")\n",
    "ax = sns.distplot(train[\"AMT_CREDIT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Distribution of AMT_INCOME_TOTAL\")\n",
    "ax = sns.distplot(train[\"AMT_ANNUITY\"].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the correlation of the positive days since birth and target\n",
    "train['DAYS_BIRTH'] = abs(train['DAYS_BIRTH'])\n",
    "train['DAYS_BIRTH'].corr(train['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "np.log(train['AMT_INCOME_TOTAL']).sns.histplot(data=train['AMT_INCOME_TOTAL'], x=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('TARGET', axis = 1)\n",
    "\n",
    "y = df['TARGET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "large amount of missing data which we considered potentially important. A balance was found to impute the missing data with Simple Imputer instead of deleting columns completely as those columns had unverified influence on the TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.NaN, strategy=\"mean\") \n",
    "idf=pd.DataFrame(imputer.fit_transform(df))\n",
    "idf.columns=df.columns\n",
    "idf.index=df.index\n",
    "df = idf\n",
    "#Impute Nan values with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "idf=pd.DataFrame(scaler.fit_transform(df))\n",
    "idf.columns=df.columns\n",
    "idf.index=df.index\n",
    "df = idf\n",
    "# summarize transformed data\n",
    "np.set_printoptions(precision=3)\n",
    "# Normalize value columns such as income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding used to turn categorical data into numerical for more accurate modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df, drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'X_Test shape: {X_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression used with low classifier to reduce over fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logreg = LogisticRegression(C = 0.0001)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_log = logreg.predict(X_test)\n",
    "\n",
    "print('Accuracy')\n",
    "print(accuracy_score(y_test, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean squared error:{mean_squared_error(y_test, y_pred_log): .2f}\")\n",
    "print(f\"Root Mean squared error: {math.sqrt(mean_squared_error(y_test, y_pred_log)) :.2f}\")\n",
    "print(f'Variance score: {r2_score(y_test, y_pred_log):.2f}')\n",
    "print(f\"Mean absolute error:{mean_absolute_error(y_test, y_pred_log): .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7d9284b47c60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1,31)\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred_knn = knn.predict(X_test)\n",
    "    scores.append(metrics.accuracy_score(y_test, y_pred_knn))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_range,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean squared error:{mean_squared_error(y_test, y_pred_knn): .2f}\")\n",
    "print(f\"Root Mean squared error: {math.sqrt(mean_squared_error(y_test, y_pred_knn)) :.2f}\")\n",
    "print(f'Variance score: {r2_score(y_test, y_pred_knn):.2f}')\n",
    "print(f\"Mean absolute error:{mean_absolute_error(y_test, y_pred_knn): .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=200, max_leaf_nodes=18, n_jobs=-1, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()  \n",
    "X_train = pca.fit_transform(X_train)  \n",
    "X_test = pca.transform(X_test)\n",
    "explained_variance = pca.explained_variance_ratio_  \n",
    "print(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X= train.drop(['TARGET'], axis=1)\n",
    "y = train['TARGET']\n",
    "\n",
    "\n",
    "\n",
    "#Oversampling the data\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y,test_size =0.2, random_state=42, stratify = y)\n",
    "\n",
    "print(X.shape)\n",
    "#XGB default settings resulted in the highest accuracy\n",
    "\n",
    "model =XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy1 = accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy is %.4f with XGBBoost'% (accuracy1)) # no missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "#get_score identifies the features that the model put the highest weight on\n",
    "importance_df = model.get_booster().get_score(importance_type='weight')\n",
    "\n",
    "features_impt_xgb = pd.DataFrame(list(importance_df.items()), columns = ['FEATURE' , 'SCORE'])\n",
    "\n",
    "print(features_impt_xgb.sort_values('SCORE', ascending = False).head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}